{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quiz2-Pang.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Es3kw9OJg1uV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##2. What type of problem are we trying to solve here? Classification or regression?\n",
        "Classification, to identify if the patient is exposed to heart disease or not."
      ]
    },
    {
      "metadata": {
        "id": "uoHG5ihng1uf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Question3\n",
        "\n",
        "import numpy as py\n",
        "import pandas as pd\n",
        "\n",
        "#Mount google drive\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "#Read the file from google drive\n",
        "data = pd.read_csv( 'gdrive/My Drive/Machine Learning Playground/heart.csv' )\n",
        "print (data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M9au-NypiLgA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Normalizing data\n",
        "# This step is really useful on improving model performance\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "cols_to_norm = ['age','chol','trestbps','thalach','oldpeak']\n",
        "data[cols_to_norm] = StandardScaler().fit_transform(data[cols_to_norm])\n",
        "\n",
        "# Create predictors\n",
        "predictors = data.drop(['target'], axis=1).values\n",
        "n_cols = predictors.shape[1] \n",
        "  \n",
        "# Create target\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "target = label_encoder.fit_transform(data.target)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(predictors, target, \n",
        "                                                    test_size=0.3 )\n",
        "\n",
        "# Build and train the model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras \n",
        "import tensorflow as tf\n",
        "\n",
        "# Input layer\n",
        "model = keras.Sequential() \n",
        "model.add(layers.Dense(100, activation='relu', input_shape = (n_cols,))) \n",
        "model.add(layers.Dense(100, activation='relu', input_shape = (n_cols,))) \n",
        "model.add(layers.Dense(100, activation='relu', input_shape = (n_cols,))) \n",
        "model.add(layers.Dense(100, activation='relu', input_shape = (n_cols,))) \n",
        "\n",
        "# Output layer\n",
        "model.add(layers.Dense(1, activation='sigmoid')) \n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) \n",
        "\n",
        "#Define early stopping to stop training if model didn't improved for 2 epochs\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping_monitor = EarlyStopping(patience=5, monitor = 'acc')\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, epochs=40, verbose=1, callbacks=[early_stopping_monitor] )\n",
        "\n",
        "# Evaluate the model\n",
        "model.evaluate(X_test, y_test )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pg4JLPJQ6-xQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Epoch 20/40\n",
        "Epoch 20/40\n",
        "212/212 [==============================] - 0s 157us/sample - loss: 0.0571 - acc: 0.9906\n",
        "Accuracy at Epoch20 is 0.9906 or 99.06%\n",
        "\n",
        "Due to the randomness during the training of model, the accuracy might be different"
      ]
    },
    {
      "metadata": {
        "id": "Z9VSkxMWrcGA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "845251af-5724-431b-a782-0545856513dd"
      },
      "cell_type": "code",
      "source": [
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "py.random.seed(seed)\n",
        "cvscores = []\n",
        "\n",
        "# define 10-fold cross validation test harness\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "\n",
        "for train, test in kfold.split(predictors, target):\n",
        "  # Input layer\n",
        "  model = keras.Sequential() \n",
        "  model.add(layers.Dense(100, activation='relu', input_shape = (n_cols,))) \n",
        "  model.add(layers.Dense(100, activation='relu')) \n",
        "\n",
        "  # Output layer\n",
        "  model.add(layers.Dense(1, activation='sigmoid')) \n",
        "\n",
        "  # Compile model\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) \n",
        "\n",
        "  # Fit the model\n",
        "  model.fit(predictors[train], target[train], epochs=40,  verbose=False )\n",
        "  # evaluate the model\n",
        "  scores = model.evaluate(predictors[test], target[test], verbose=0)\n",
        "  print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "  cvscores.append(scores[1] * 100)\n",
        "\n",
        "print(\"%.2f%% (+/- %.2f%%)\" % (py.mean(cvscores), py.std(cvscores)))"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acc: 78.69%\n",
            "acc: 81.97%\n",
            "acc: 78.69%\n",
            "acc: 86.67%\n",
            "acc: 81.67%\n",
            "81.54% (+/- 2.92%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F5S_ktvZg1ui",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#4. What is underfitting and overfitting?\n",
        "Underfitting : the model is not train enough and unable to identify / detect any useful pattern from the input data\n",
        "Overfitting  : the model is over trained / fully adapted to the input data, thus it is very sensitive to the changes of input data and not really useful in prediction where the input data is differ to the training data.  "
      ]
    },
    {
      "metadata": {
        "id": "iTVPeqgGg1uj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#5. Explain how you can reduce the overfitting of the model\n",
        "To reduce overfitting of data, K-fold cross validation / splitting the data into training and validation will help to validate the model. By doing K-fold cross validation, data will be splitted into multiple set of test & train and repeated multiple times. Each time, the test & train data will be shuffled, thus preventing the model from overfitting by introducing data in various form.\n",
        "\n",
        "Splitting the training data into training and validation only application at big dataset,"
      ]
    }
  ]
}